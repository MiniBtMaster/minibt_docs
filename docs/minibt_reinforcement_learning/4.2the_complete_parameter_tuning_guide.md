# minibt 强化学习参数配置指南

## 概述

minibt 框架提供了完整的强化学习解决方案，通过 `set_model_params` 方法可以灵活配置强化学习训练的各个方面。以下是对主要参数配置的详细介绍：

## 1. 核心组件模块

```python
__all__ = ["Loss", "Optim", "LrScheduler", "SWALR", "Activation", "Norm"]
```

### 1.1 损失函数 (Loss)
```python
# 常用损失函数示例
self.set_model_params(
    Loss=Loss.MSELoss(reduction="none"),           # 回归任务
    Loss=Loss.CrossEntropyLoss(),                  # 分类任务
    Loss=Loss.HuberLoss(),                         # 对异常值鲁棒
    Loss=Loss.BCEWithLogitsLoss(),                 # 二分类任务
)
```

### 1.2 优化器 (Optim)
```python
# 优化器配置示例
self.set_model_params(
    Optim=Optim.AdamW(eps=1e-5, weight_decay=1e-4),  # 推荐用于Transformer
    Optim=Optim.SGD(momentum=0.9),                   # 简单任务
    Optim=Optim.OGSignMuon(),                        # 自定义优化器
)
```

### 1.3 学习率调度器 (LrScheduler)
```python
# 学习率调度示例
self.set_model_params(
    LrScheduler=LrScheduler.CosineAnnealingLR(T_max=100),  # 余弦退火
    LrScheduler=LrScheduler.StepLR(step_size=30, gamma=0.1),  # 阶梯下降
    LrScheduler=LrScheduler.OneCycleLR(max_lr=0.1, total_steps=1000),  # 单周期
)
```

### 1.4 激活函数 (Activation)
```python
# 激活函数配置
self.set_model_params(
    Activation=Activation.Tanh10(),     # 强化学习推荐
    Activation=Activation.ReLU(),       # 通用隐藏层
    Activation=Activation.GELU(),       # Transformer模型
)
```

### 1.5 归一化层 (Norm)
```python
# 归一化配置
self.set_model_params(
    Norm=Norm.LayerNorm,                # 序列数据
    Norm=Norm.InstanceNorm1d,           # 时间序列
)
```

## 2. 环境与算法配置

### 2.1 基础环境设置
```python
self.set_model_params(
    agent=Agents.AgentPPO,              # 算法选择：PPO, SAC, DDPG等
    train=True,                         # 训练模式
    continue_train=False,               # 是否继续训练
    random_policy_test=False,           # 随机策略测试
    window_size=10,                     # 状态观察窗口
)
```

### 2.2 环境参数
```python
self.set_model_params(
    env_name="MyTradingEnv",            # 环境名称
    num_envs=1,                         # 并行环境数量
    max_step=0,                         # 最大步数（0=自动计算）
    state_dim=0,                        # 状态维度（0=自动计算）
    action_dim=1,                       # 动作维度
    if_discrete=False,                  # 连续/离散动作空间
)
```

## 3. 训练超参数配置

### 3.1 训练控制参数
```python
self.set_model_params(
    break_step=1e6,                     # 训练中断步数
    batch_size=128,                     # 批次大小
    horizon_len=2048,                   # 经验收集长度
    buffer_size=None,                   # 经验缓冲区大小
    repeat_times=8.0,                   # 策略更新重复次数
)
```

### 3.2 奖励与折扣参数
```python
self.set_model_params(
    gamma=0.985,                        # 折扣因子
    reward_scale=1.0,                   # 奖励缩放
    lambda_gae_adv=0.95,                # GAE优势估计
    lambda_entropy=0.01,                # 熵奖励系数
)
```

### 3.3 网络结构参数
```python
self.set_model_params(
    net_dims=(64, 32),                  # 神经网络隐藏层维度
    learning_rate=6e-5,                 # 学习率
    weight_decay=1e-4,                  # 权重衰减
    clip_grad_norm=0.5,                 # 梯度裁剪
    dropout_rate=0.0,                   # Dropout比率
    bias=True,                          # 是否使用偏置
)
```

## 4. 硬件与并行配置

### 4.1 硬件设置
```python
self.set_model_params(
    gpu_id=0,                           # GPU设备ID
    num_workers=4,                      # 数据加载工作线程
    num_threads=8,                      # 计算线程数
    learner_gpu_ids=(),                 # 学习者GPU列表
)
```

### 4.2 随机种子
```python
self.set_model_params(
    random_seed=42,                     # 随机种子（确保可重复性）
)
```

## 5. 文件与保存配置

### 5.1 模型保存
```python
self.set_model_params(
    cwd="./models/my_strategy",         # 工作目录
    if_remove=True,                     # 是否移除旧模型
    if_keep_save=True,                  # 是否保存检查点
    save_gap=8,                         # 模型保存间隔
    file_extension=".pth",              # 模型文件扩展名
)
```

### 5.2 评估配置
```python
self.set_model_params(
    eval_times=3,                       # 评估次数
    eval_per_step=2e4,                  # 评估间隔步数
    break_score=np.inf,                 # 训练中断分数阈值
)
```

## 6. 高级配置选项

### 6.1 PPO算法特定参数
```python
self.set_model_params(
    ratio_clip=0.25,                    # PPO裁剪比率
    state_value_tau=0.0,                # 状态价值平滑
    soft_update_tau=5e-3,               # 目标网络软更新
)
```

### 6.2 优化器高级参数
```python
self.set_model_params(
    eps=1e-5,                           # 数值稳定性
    momentum=0.9,                       # 动量参数
    lr_decay_rate=0.999,                # 学习率衰减率
)
```

## 7. 完整配置示例

```python
class MyRLStrategy(Strategy):
    rl = True
    
    def __init__(self):
        # 数据和技术指标初始化
        self.data = self.get_kline(LocalDatas.v2509_60_2)
        self.ma1 = self.data.close.sma(3)
        self.ma2 = self.data.close.sma(5)
        # ... 更多指标
        
        # 强化学习完整配置
        self.set_model_params(
            # 算法和环境
            agent=Agents.AgentPPO,
            train=True,
            window_size=10,
            
            # 网络结构
            net_dims=(128, 64),
            learning_rate=3e-4,
            weight_decay=1e-4,
            
            # 训练参数
            batch_size=256,
            horizon_len=2048,
            gamma=0.99,
            break_step=1e5,
            
            # 自定义组件
            Optim=Optim.AdamW(eps=1e-5, weight_decay=1e-4),
            Activation=Activation.Tanh10(),
            Loss=Loss.MSELoss(reduction="none"),
            
            # 硬件配置
            gpu_id=0,
            random_seed=42,
            
            # 文件保存
            cwd="./models/my_strategy",
            if_remove=True,
        )
```

## 8. 参数调优建议

### 8.1 学习率选择
- **Adam/AdamW**: 1e-4 到 1e-3
- **SGD**: 0.01 到 0.1
- **PPO**: 通常使用较小的学习率 (3e-4)

### 8.2 网络结构
- **简单任务**: (64, 32) 或 (128, 64)
- **复杂任务**: (256, 128, 64) 或更深网络
- **过拟合**: 增加 dropout_rate (0.1-0.3)

### 8.3 训练参数
- **折扣因子**: 0.95-0.99 (长期规划用更高值)
- **批次大小**: 128-512 (根据内存调整)
- **经验长度**: 1024-4096 (平衡学习效率)

## 9. 注意事项

1. **自动计算维度**: state_dim 和 action_dim 设为 0 时自动计算
2. **环境名称**: 不指定时使用策略类名 + "Env"
3. **随机种子**: 设置固定种子确保实验可重复
4. **GPU配置**: 多GPU训练使用 learner_gpu_ids
5. **模型保存**: if_remove=True 会清理旧模型，谨慎使用

通过合理配置这些参数，您可以构建高效的强化学习交易策略，充分利用 minibt 框架的强大功能。

> 风险提示：本文涉及的交易策略、代码示例均为技术演示、教学探讨，仅用于展示逻辑思路，绝不构成任何投资建议、操作指引或决策依据 。金融市场复杂多变，存在价格波动、政策调整、流动性等多重风险，历史表现不预示未来结果。任何交易决策均需您自主判断、独立承担责任 —— 若依据本文内容操作，盈亏后果概由自身承担。请务必充分评估风险承受能力，理性对待市场，谨慎做出投资选择。