# minibt æ¡†æ¶å¼ºåŒ–å­¦ä¹ ä½¿ç”¨æŒ‡å—

minibt æ¡†æ¶æä¾›äº†å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒå¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†é‡åŒ–äº¤æ˜“ç­–ç•¥è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç» minibt æ¡†æ¶ä¸­å¼ºåŒ–å­¦ä¹ åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•ã€‚

## 1. åŸºç¡€æ¦‚å¿µ

### 1.1 å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–äº¤æ˜“ä¸­çš„åº”ç”¨

åœ¨é‡åŒ–äº¤æ˜“ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å°†äº¤æ˜“å†³ç­–å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼š
- **çŠ¶æ€ï¼ˆStateï¼‰**ï¼šå¸‚åœºç‰¹å¾ã€æŠ€æœ¯æŒ‡æ ‡ã€æŒä»“ä¿¡æ¯ç­‰
- **åŠ¨ä½œï¼ˆActionï¼‰**ï¼šä¹°å…¥ã€å–å‡ºã€æŒæœ‰ç­‰äº¤æ˜“å†³ç­–
- **å¥–åŠ±ï¼ˆRewardï¼‰**ï¼šåŸºäºäº¤æ˜“ç›ˆäºè®¡ç®—çš„å›æŠ¥

### 1.2 minibt RL æ ¸å¿ƒç»„ä»¶

```python
from minibt import *
from minibt.rl_utils import *

class MyStrategy(Strategy):
    rl = True  # å¯ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å¼
    
    def __init__(self):
        # 1. æ•°æ®å‡†å¤‡å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
        self.data = self.get_kline(LocalDatas.v2601_60)
        self.ma1 = self.data.close.sma(3)
        self.ma2 = self.data.close.sma(5)
        # ... æ›´å¤šæŒ‡æ ‡
        
        # 2. é…ç½®å¼ºåŒ–å­¦ä¹ å‚æ•°
        self.set_model_params(
            Agents.AgentPPO,  # ä½¿ç”¨PPOç®—æ³•
            train=True,       # è®­ç»ƒæ¨¡å¼
            break_step=1e5,   # è®­ç»ƒæ­¥æ•°
            # ... æ›´å¤šå‚æ•°
        )
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒçŠ¶æ€"""
        self.current_step = self.min_start_length
        self.pos = 0
        return self._get_observation(), {}
    
    def _get_observation(self):
        """è·å–è§‚æµ‹çŠ¶æ€"""
        obs = self.signal_features[self.current_step+1-self.window_size:self.current_step+1]
        if self.train:
            return self.data_enhancement(obs)  # è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®å¢å¼º
        return obs.flatten()
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›ç»“æœ"""
        # åŠ¨ä½œå¤„ç†é€»è¾‘
        processed_action = self._process_action(action)
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(processed_action)
        
        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        done = self.current_step >= self.max_step
        
        return self._get_observation(), reward, done, False, {}
```

## 2. ç‰¹å¾å·¥ç¨‹ä¸æŒ‡æ ‡è·å–

### 2.1 æŠ€æœ¯æŒ‡æ ‡è®¡ç®—

```python
class OwenStrategy(Strategy):
    rl = True
    
    def __init__(self):
        # è·å–åŸºç¡€æ•°æ®
        self.data = self.get_kline(LocalDatas.v2509_60_2)
        
        # ç§»åŠ¨å¹³å‡çº¿ç³»åˆ—
        self.ma1 = self.data.close.sma(3)    # 3å‘¨æœŸå‡çº¿
        self.ma2 = self.data.close.sma(5)    # 5å‘¨æœŸå‡çº¿
        self.ma3 = self.data.close.sma(8)    # 8å‘¨æœŸå‡çº¿
        self.ma4 = self.data.close.sma(13)   # 13å‘¨æœŸå‡çº¿
        self.ma5 = self.data.close.sma(21)   # 21å‘¨æœŸå‡çº¿
        self.ma6 = self.data.close.sma(34)   # 34å‘¨æœŸå‡çº¿
        self.ma7 = self.data.close.sma(55)   # 55å‘¨æœŸå‡çº¿
        
        # Z-score æ ‡å‡†åŒ–
        self.zscore10 = self.data.close.zscore(10)  # 10å‘¨æœŸZ-score
        self.zscore20 = self.data.close.zscore(20)  # 20å‘¨æœŸZ-score
        
        # RSI ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡
        self.rsi = self.data.close.rsi()
        
        # EBSW æŒ‡æ ‡
        self.ebsw = self.data.close.ebsw()
        
        # ATR å¹³å‡çœŸå®æ³¢å¹…
        self.atr1 = self.data.atr(10)  # 10å‘¨æœŸATR
        self.atr2 = self.data.atr(20)  # 20å‘¨æœŸATR
        
        # æ ‡å‡†å·®
        self.std1 = self.data.close.stdev(10)  # 10å‘¨æœŸæ ‡å‡†å·®
        self.std2 = self.data.close.stdev(20)  # 20å‘¨æœŸæ ‡å‡†å·®
        
        # PVI æ­£æˆäº¤é‡æŒ‡æ ‡
        self.pvi1 = self.data.pvi(10)  # 10å‘¨æœŸPVI
        self.pvi2 = self.data.pvi(20)  # 20å‘¨æœŸPVI
        
        # CCI å•†å“é€šé“æŒ‡æ ‡
        self.cci1 = self.data.close.cci(10)  # 10å‘¨æœŸCCI
        self.cci2 = self.data.close.cci(20)  # 20å‘¨æœŸCCI
        
        # ADX å¹³å‡è¶‹å‘æŒ‡æ•°
        self.adx1 = self.data.close.adx(10).iloc[:, 0]  # 10å‘¨æœŸADX
        self.adx2 = self.data.close.adx(20).iloc[:, 0]  # 20å‘¨æœŸADX
```

### 2.2 ç‰¹å¾å¤„ç†ä¸å½’ä¸€åŒ–

```python
# é…ç½®ç‰¹å¾å¤„ç†å‚æ•°
strategy.set_process_quant_features(
    normalize_method='robust',      # ä½¿ç”¨é²æ£’å½’ä¸€åŒ–ï¼ˆæŠ—å¼‚å¸¸å€¼ï¼‰
    rolling_window=60,              # æ»šåŠ¨çª—å£å¤§å°
    feature_range=(-1, 1),          # ç‰¹å¾ç¼©æ”¾èŒƒå›´
    use_log_transform=True,         # ä½¿ç”¨å¯¹æ•°å˜æ¢
    handle_outliers="clip",         # å¼‚å¸¸å€¼å¤„ç†æ–¹å¼ï¼šæˆªæ–­
    pca_n_components=1.0,           # PCAé™ç»´ï¼ˆ1.0è¡¨ç¤ºä¿ç•™å…¨éƒ¨ç‰¹å¾ï¼‰
    target_returns=None             # ç›®æ ‡æ”¶ç›Šç‡ï¼ˆå¯é€‰ï¼Œç”¨äºç‰¹å¾é€‰æ‹©ï¼‰
)

# è·å–å¤„ç†åçš„ç‰¹å¾
features = strategy.get_signal_features()
```

**ç‰¹å¾å¤„ç†æ–¹æ³•è¯´æ˜ï¼š**

- **å½’ä¸€åŒ–æ–¹æ³•**ï¼š
  - `standard`ï¼šæ ‡å‡†å½’ä¸€åŒ–ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰
  - `robust`ï¼šé²æ£’å½’ä¸€åŒ–ï¼ˆä¸­ä½æ•°ä¸º0ï¼Œå››åˆ†ä½è·ä¸º1ï¼ŒæŠ—å¼‚å¸¸å€¼ï¼‰
  - `minmax`ï¼šæœ€å°æœ€å¤§å½’ä¸€åŒ–ï¼ˆç¼©æ”¾åˆ°æŒ‡å®šèŒƒå›´ï¼‰
  - `rolling`ï¼šæ»šåŠ¨çª—å£å½’ä¸€åŒ–ï¼ˆé¿å…æœªæ¥æ•°æ®æ³„éœ²ï¼‰

- **å¼‚å¸¸å€¼å¤„ç†**ï¼š
  - `clip`ï¼šä½¿ç”¨å››åˆ†ä½æ³•æˆªæ–­å¼‚å¸¸å€¼
  - `mark`ï¼šæ–°å¢å¼‚å¸¸å€¼æ ‡è®°ç‰¹å¾

- **ç‰¹å¾å˜æ¢**ï¼š
  - å¯¹æ•°å˜æ¢ï¼šå‹ç¼©é•¿å°¾åˆ†å¸ƒ
  - PCAé™ç»´ï¼šå‡å°‘ç‰¹å¾ç»´åº¦ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯

### 2.3 æ•°æ®å¢å¼º

```python
def data_enhancement(self, obs: np.ndarray, rate: float = 0.5) -> np.ndarray:
    """
    æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ
    
    å‚æ•°ï¼š
        obs: è¾“å…¥ç‰¹å¾æ•°æ®
        rate: åº”ç”¨å¢å¼ºçš„æ¦‚ç‡ï¼ˆé»˜è®¤0.5ï¼‰
    
    è¿”å›ï¼š
        å¢å¼ºåæˆ–åŸå§‹çš„ç‰¹å¾æ•°ç»„
    """
    # 50%æ¦‚ç‡åº”ç”¨æ•°æ®å¢å¼º
    if np.random.rand() < rate:
        augment_func = np.random.choice(self._data_enhancement_funcs)
        return augment_func(obs)
    return obs.flatten()
```

## 3. å¼ºåŒ–å­¦ä¹ æ¨¡å‹é…ç½®

### 3.1 åŸºç¡€æ¨¡å‹é…ç½®

```python
class OwenStrategy(Strategy):
    def __init__(self):
        # ... æŒ‡æ ‡è®¡ç®— ...
        
        # é…ç½®å¼ºåŒ–å­¦ä¹ æ¨¡å‹å‚æ•°
        self.set_model_params(
            agent=Agents.AgentPPO,           # ä½¿ç”¨PPOç®—æ³•
            train=True,                      # è®­ç»ƒæ¨¡å¼
            continue_train=False,            # ä¸ç»§ç»­è®­ç»ƒ
            random_policy_test=False,        # ä¸è¿›è¡Œéšæœºç­–ç•¥æµ‹è¯•
            window_size=10,                  # çŠ¶æ€è§‚å¯Ÿçª—å£å¤§å°
            env_name="OwenTradingEnv",       # ç¯å¢ƒåç§°
            num_envs=1,                      # å¹¶è¡Œç¯å¢ƒæ•°é‡
            max_step=0,                      # æœ€å¤§æ­¥æ•°ï¼ˆ0è¡¨ç¤ºä½¿ç”¨æ•°æ®é›†é•¿åº¦ï¼‰
            state_dim=0,                     # çŠ¶æ€ç»´åº¦ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰
            action_dim=1,                    # åŠ¨ä½œç»´åº¦
            if_discrete=False,               # è¿ç»­åŠ¨ä½œç©ºé—´
            break_step=1e5,                  # è®­ç»ƒä¸­æ–­æ­¥æ•°
            batch_size=128,                  # æ‰¹æ¬¡å¤§å°
            horizon_len=2048,                # ç»éªŒæ”¶é›†é•¿åº¦
            buffer_size=None,                # ç¼“å†²åŒºå¤§å°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æ•°æ®é›†é•¿åº¦ï¼‰
            repeat_times=8.0,                # ç­–ç•¥æ›´æ–°é‡å¤æ¬¡æ•°
            if_use_per=False,                # ä¸ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾
            gamma=0.985,                     # æŠ˜æ‰£å› å­
            reward_scale=1.0,                # å¥–åŠ±ç¼©æ”¾å› å­
            net_dims=(64, 32),               # ç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦
            learning_rate=6e-5,              # å­¦ä¹ ç‡
            weight_decay=1e-4,               # æƒé‡è¡°å‡
            clip_grad_norm=0.5,              # æ¢¯åº¦è£å‰ªé˜ˆå€¼
            save_gap=8,                      # æ¨¡å‹ä¿å­˜é—´éš”
            ratio_clip=0.25,                 # PPOè£å‰ªæ¯”ç‡
            lambda_gae_adv=0.95,             # GAEä¼˜åŠ¿ä¼°è®¡å‚æ•°
            lambda_entropy=0.01,             # ç†µå¥–åŠ±ç³»æ•°
            gpu_id=0,                        # GPUè®¾å¤‡ID
            random_seed=42,                  # éšæœºç§å­
            cwd="./models/owen",             # å·¥ä½œç›®å½•
            if_remove=True,                  # ç§»é™¤æ—§æ¨¡å‹æ–‡ä»¶
            eval_times=3,                    # è¯„ä¼°æ¬¡æ•°
            eval_per_step=2e4,               # è¯„ä¼°é—´éš”æ­¥æ•°
            Optim=Optim.OGSignMuon(),        # è‡ªå®šä¹‰ä¼˜åŒ–å™¨
            dropout_rate=0.2,                # Dropoutæ¯”ç‡
        )
        
        # è°ƒæ•´æœ€å¤§æ­¥æ•°ï¼ˆé¿å…è¶Šç•Œï¼‰
        self.max_step -= 4
        
        # äº¤æ˜“ç›¸å…³å‚æ•°
        self.hoding_day = 0
        self.last_action = 0
        self.data.price_tick = 1.0
        self.data.volume_multiple = 5.0
        self.data.fixed_commission = 0.0
```

### 3.2 æ”¯æŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•

minibt æ¡†æ¶æ”¯æŒå¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š

```python
from minibt.elegantrl.agents import Agents

# æ”¯æŒçš„ç®—æ³•åŒ…æ‹¬ï¼š
# - Agents.AgentPPO: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
# - Agents.AgentSAC: è½¯æ¼”å‘˜è¯„è®ºå®¶
# - Agents.AgentTD3: åŒå»¶è¿Ÿæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦
# - Agents.AgentDDPG: æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦
# - Agents.AgentA2C: ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå®¶
# - Agents.AgentDQN: æ·±åº¦Qç½‘ç»œ
```

### 3.3 è‡ªå®šä¹‰ç½‘ç»œç»„ä»¶

```python
self.set_model_params(
    # è‡ªå®šä¹‰æŸå¤±å‡½æ•°
    Loss=Loss.MSELoss(reduction="none"),
    
    # è‡ªå®šä¹‰ä¼˜åŒ–å™¨
    Optim=Optim.OGSignMuon(eps=1e-5, weight_decay=1e-4),
    
    # è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°
    Activation=Activation.Tanh10(),
    
    # è‡ªå®šä¹‰å½’ä¸€åŒ–å±‚
    Norm=Norm.LayerNorm,
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    LrScheduler=LrScheduler.ExponentialLR,
    
    # éšæœºæƒé‡å¹³å‡
    SWA=SWA.StochasticWeightAveraging,
)
```

## 4. ç¯å¢ƒäº¤äº’æ–¹æ³•

### 4.1 çŠ¶æ€è§‚æµ‹è·å–

```python
def _get_observation(self) -> np.ndarray:
    """
    è·å–å½“å‰çŠ¶æ€è§‚æµ‹
    
    è¿”å›ï¼š
        å¤„ç†åçš„ç‰¹å¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(state_dim,)
    """
    # è·å–çª—å£å†…çš„ç‰¹å¾æ•°æ®
    obs = self.signal_features[
        self.current_step + 1 - self.window_size: 
        self.current_step + 1
    ]
    
    # è®­ç»ƒæ—¶åº”ç”¨æ•°æ®å¢å¼ºï¼Œæ¨ç†æ—¶ç›´æ¥å±•å¹³
    if self.train:
        return self.data_enhancement(obs, rate=0.5)
    return obs.flatten()
```

### 4.2 åŠ¨ä½œå¤„ç†

```python
def _process_action(self, action):
    """å¤„ç†åŠ¨ä½œ"""
    # å°†è¿ç»­åŠ¨ä½œ [-1, 1] æ˜ å°„åˆ°ç¦»æ•£åŠ¨ä½œ [-3, -2, -1, 1, 2, 3]
    x = action[0]
    # åŸåŒºé—´
    x_min, x_max = -1, 1
    # ç›®æ ‡åŒºé—´
    y_min, y_max = 0, 5

    # é¦–å…ˆçº¿æ€§æ˜ å°„åˆ°[1, 6]åŒºé—´
    y = y_min + (x - x_min) * (y_max - y_min) / (x_max - x_min)

    # å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œç¡®ä¿ä¸ä¼šè¶…å‡ºèŒƒå›´
    if y < 0:
        a = 0
    elif y > 5:
        a = 5
    else:
        # å››èˆäº”å…¥å–æ•´æ•°
        a = round(y)  
    action -= 3
    if action >= 0:
        return action+1
    return action # æ˜ å°„åˆ°[-3, -2, -1, 1, 2, 3]åŒºé—´
```

### 4.3 å¥–åŠ±è®¡ç®—

```python
def step(self, action):
    """
    æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›ç¯å¢ƒåé¦ˆ
    
    å‚æ•°ï¼š
        action: æ¨¡å‹è¾“å‡ºçš„åŠ¨ä½œ
    
    è¿”å›ï¼š
        observation: æ–°çš„çŠ¶æ€è§‚æµ‹
        reward: å¥–åŠ±å€¼
        done: æ˜¯å¦ç»“æŸ
        truncated: æ˜¯å¦è¢«æˆªæ–­
        info: é™„åŠ ä¿¡æ¯
    """
    reward, done = 0.0, False
    
    # å¤„ç†åŠ¨ä½œ
    processed_action = self._get_action(action)
    
    # è®­ç»ƒæ¨¡å¼ä¸‹çš„å¥–åŠ±è®¡ç®—
    if self.train:
        if processed_action > 0:  # åšå¤šåŠ¨ä½œ [1, 2, 3]
            # ä½¿ç”¨æœªæ¥ä»·æ ¼å˜åŒ–ä½œä¸ºå¥–åŠ±
            reward = self.long_prices[processed_action - 1][self.current_step]
        elif processed_action < 0:  # åšç©ºåŠ¨ä½œ [-1, -2, -3]
            reward = self.short_prices[-processed_action - 1][self.current_step]
        # processed_action == 0 æ—¶ reward ä¿æŒ 0
    
    # å›æµ‹æ¨¡å¼ä¸‹çš„å®é™…äº¤æ˜“é€»è¾‘
    else:
        if self.data.position == 0:  # å½“å‰æ— æŒä»“
            if processed_action > 0:  # åšå¤š
                self.data.buy()
                self.hoding_day = processed_action
                self.last_action = processed_action
            elif processed_action < 0:  # åšç©º
                self.data.sell()
                self.hoding_day = -processed_action
                self.last_action = -processed_action
        
        elif self.data.position > 0:  # å½“å‰æŒå¤šä»“
            if processed_action > 0:
                self.hoding_day = min(processed_action, self.hoding_day)
            self.hoding_day -= 1
            if self.hoding_day <= 0:  # æŒä»“æœŸæ»¡ï¼Œå¹³ä»“
                self.data.sell()
        
        else:  # å½“å‰æŒç©ºä»“
            if processed_action < 0:
                self.hoding_day = min(-processed_action, self.hoding_day)
            self.hoding_day -= 1
            if self.hoding_day <= 0:  # æŒä»“æœŸæ»¡ï¼Œå¹³ä»“
                self.data.buy()
    
    # æ›´æ–°æ­¥æ•°
    self.current_step += 1
    
    # æ£€æŸ¥æ˜¯å¦ç»“æŸ
    if self.current_step >= self.max_step:
        done = True
    
    return self._get_observation(), reward, done, False, {}
```

## 5. è®­ç»ƒä¸æ¨ç†

### 5.1 å¯åŠ¨è®­ç»ƒ

```python
# é…ç½®å®Œæˆåå¯åŠ¨è®­ç»ƒ
strategy.train_agent()

# æˆ–è€…æ˜¾å¼è°ƒç”¨
from minibt.elegantrl.train.run import train_agent
train_agent(strategy._rl_config, True)
```

### 5.2 åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

```python
# åœ¨ç­–ç•¥åˆå§‹åŒ–ååŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
strategy.load_agent()

# åŠ è½½åçš„æ¨¡å‹å¯ä»¥é€šè¿‡ strategy.actor è®¿é—®
# åœ¨stepæ–¹æ³•ä¸­ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è¿›è¡Œæ¨ç†
```

### 5.3 éšæœºç­–ç•¥æµ‹è¯•

```python
# æµ‹è¯•éšæœºç­–ç•¥ï¼ŒéªŒè¯ç¯å¢ƒè®¾ç½®
strategy.random_policy_test()
```

## 6. å®Œæ•´ç¤ºä¾‹

```python
from minibt import *
from minibt.rl_utils import *


class CompleteRLStrategy(Strategy):
    rl = True

    def __init__(self):
        # åŸºç¡€è®¾ç½®
        self.min_start_length = 300
        self.data = self.get_kline(LocalDatas.v2509_60_3)
        self.data.height = 500

        # æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
        self._setup_technical_indicators()

        # é…ç½®å¼ºåŒ–å­¦ä¹ 
        self._setup_rl_config()

        # äº¤æ˜“å‚æ•°
        self._setup_trading_params()

    def _setup_technical_indicators(self):
        """è®¾ç½®æŠ€æœ¯æŒ‡æ ‡"""
        # ç§»åŠ¨å¹³å‡çº¿
        self.ma1 = self.data.close.sma(3)
        self.ma2 = self.data.close.sma(5)
        self.ma3 = self.data.close.sma(8)
        self.ma4 = self.data.close.sma(13)
        self.ma5 = self.data.close.sma(21)

        # éœ‡è¡æŒ‡æ ‡
        self.rsi = self.data.close.rsi()
        self.cci = self.data.close.cci(14)
        self.adx = self.data.close.adx(14).iloc[:, 0]
        self.pvi = self.data.pvi(14)
        self.ebsw = self.data.close.ebsw()
        self.zscore = self.data.close.zscore(14)

        # æ³¢åŠ¨ç‡æŒ‡æ ‡
        self.atr = self.data.atr(14)
        self.std = self.data.close.stdev(14)

        self.btindicatordataset.isplot = False

        # é…ç½®ç‰¹å¾å¤„ç†
        self.set_process_quant_features(
            normalize_method='robust',
            use_log_transform=True,
            handle_outliers="clip"
        )

        # ä»·æ ¼ç›®æ ‡ï¼ˆç”¨äºå¥–åŠ±è®¡ç®—ï¼‰
        # å¤šå¤´æœªæ¥1-3å‘¨æœŸçš„æ”¶ç›Š
        self.long_prices = [
            self.data.pandas_object.close.diff().shift(-i).values
            for i in range(1, 4)
        ]
        # ç©ºå¤´æœªæ¥1-3å‘¨æœŸçš„æ”¶ç›Š
        self.short_prices = [-price for price in self.long_prices]

    def _setup_rl_config(self):
        """é…ç½®å¼ºåŒ–å­¦ä¹ å‚æ•°"""
        self.set_model_params(
            Agents.AgentPPO,
            train=True,
            break_step=3e5,
            if_remove=True,
            action_dim=1,
            if_discrete=False,
            Optim=Optim.OGSignMuon(),
        )
        # è°ƒæ•´æ­¥æ•°é™åˆ¶,æœ€å3æ ¹Kçº¿æ— æœªæ¥æ”¶ç›Šï¼Œå³å€’æ•°ç¬¬å››æ ¹Kçº¿è®­ç»ƒç»“æŸ
        self.max_step -= 4

    def _setup_trading_params(self):
        """è®¾ç½®äº¤æ˜“å‚æ•°"""
        self.hoding_day = 0  # æŒä»“å‘¨æœŸ
        self.last_action = 0  # æœ€åäº¤æ˜“çš„åŠ¨ä½œ
        self.data.price_tick = 1.0  # æœ€å°æ³¢åŠ¨å•ä½1.
        self.data.volume_multiple = 5.0  # æœ€å°ä¹˜æ•°5.
        self.data.fixed_commission = 0.0  # æ— æ‰‹ç»­è´¹ç”¨

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = self.min_start_length
        self.pos = 0
        self.hoding_day = 0
        self.last_action = 0
        return self._get_observation(), {}

    def _get_observation(self):
        """è·å–è§‚æµ‹çŠ¶æ€"""
        obs = self.signal_features[
            self.current_step + 1 - self.window_size:
            self.current_step + 1
        ]
        if self.train:  # æœ‰30%æ¦‚ç‡ä½¿ç”¨æ•°æ®å¢å¼º
            return self.data_enhancement(obs, rate=0.3)
        return obs.flatten()

    def _process_action(self, action):
        """å¤„ç†åŠ¨ä½œ"""
        # å°†è¿ç»­åŠ¨ä½œ [-1, 1] æ˜ å°„åˆ°ç¦»æ•£åŠ¨ä½œ [-3, -2, -1, 1, 2, 3]
        x = action[0]
        # åŸåŒºé—´
        x_min, x_max = -1, 1
        # ç›®æ ‡åŒºé—´
        y_min, y_max = 0, 5

        # é¦–å…ˆçº¿æ€§æ˜ å°„åˆ°[1, 6]åŒºé—´
        y = y_min + (x - x_min) * (y_max - y_min) / (x_max - x_min)

        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œç¡®ä¿ä¸ä¼šè¶…å‡ºèŒƒå›´
        if y < 0:
            action = 0
        elif y > 5:
            action = 5
        else:
            # å››èˆäº”å…¥å–æ•´æ•°
            action = round(y)
        action -= 3
        if action >= 0:
            return action+1
        return action  # æ˜ å°„åˆ°[-3, -2, -1, 1, 2, 3]åŒºé—´

    def step(self, action):
        """ç¯å¢ƒæ­¥è¿›"""
        reward = 0.0
        processed_action = self._process_action(action)

        # è®­ç»ƒå¥–åŠ±è®¡ç®—
        if self.train:
            if processed_action > 0:
                reward = self.long_prices[processed_action -
                                          1][self.current_step]
            elif processed_action < 0:
                reward = self.short_prices[-processed_action -
                                           1][self.current_step]
        # éè®­ç»ƒæ—¶è¿›è¡Œå›æµ‹
        else:
            if self.data.position == 0:
                if processed_action > 0:
                    self.data.buy()
                    self.hoding_day = processed_action
                    self.last_action = processed_action
                else:
                    self.data.sell()
                    self.hoding_day = -processed_action
                    self.last_action = -processed_action
            elif self.data.position > 0:
                if processed_action > 0:
                    self.hoding_day = min(processed_action, self.hoding_day)
                self.hoding_day -= 1
                if self.hoding_day <= 0:
                    self.data.sell()
            else:
                if processed_action < 0:
                    self.hoding_day = min(-processed_action, self.hoding_day)
                self.hoding_day -= 1
                if self.hoding_day <= 0:
                    self.data.buy()

        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        done = self.current_step >= self.max_step

        return self._get_observation(), reward, done, False, {}


if __name__ == "__main__":
    # åˆ›å»ºå¹¶è¿è¡Œç­–ç•¥
    Bt().run()
```
```python
# éªŒè¯æ—¶å°†trainè®¾ç½®ä¸ºFalseè¿è¡Œå³å¯
self.set_model_params(
    Agents.AgentPPO,
    train=False,
    break_step=3e5,
    if_remove=True,
    action_dim=1,
    if_discrete=False,
    Optim=Optim.OGSignMuon(),
)
```
# å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒä¸éªŒè¯ç»“æœåˆ†æ

## ğŸ“Š è®­ç»ƒè¿‡ç¨‹æ€»ç»“
![](../plot/4_1_1.png)
![](../plot/4_1_2.png)
### è®­ç»ƒè¡¨ç°æŒ‡æ ‡åˆ†æ
ä»è®­ç»ƒæ—¥å¿—å¯ä»¥çœ‹å‡ºï¼Œç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºä»¥ä¸‹ç‰¹ç‚¹ï¼š

**ç§¯æè¿›å±•ï¼š**
- âœ… **å¹³å‡å›æŠ¥æŒç»­æå‡**ï¼šä»åˆå§‹çš„228.67ç¨³æ­¥å¢é•¿åˆ°792.33ï¼Œå¢é•¿çº¦247%
- âœ… **æœŸæœ›å›æŠ¥æ”¹å–„**ï¼šä»-1.42æå‡è‡³-1.07ï¼Œè¡¨æ˜ç­–ç•¥å­¦ä¹ æ•ˆæœè‰¯å¥½
- âœ… **è®­ç»ƒç¨³å®šæ€§**ï¼šè¯„è®ºè€…ç›®æ ‡ï¼ˆobjVï¼‰å’Œè¡ŒåŠ¨è€…ç›®æ ‡ï¼ˆobjAï¼‰ä¿æŒç›¸å¯¹ç¨³å®š
- âœ… **è®­ç»ƒæ•ˆç‡**ï¼šåœ¨189ç§’å†…å®Œæˆ289,000æ­¥è®­ç»ƒï¼Œè®­ç»ƒè¿‡ç¨‹é«˜æ•ˆ

**è®­ç»ƒå‚æ•°ï¼š**
- **ç®—æ³•**ï¼šPPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰
- **è®­ç»ƒæ­¥æ•°**ï¼š289,000æ­¥
- **è®­ç»ƒæ—¶é—´**ï¼š189ç§’
- **æ¨¡å‹ä¿å­˜**ï¼šæˆåŠŸä¿å­˜è‡³ `./CompletedLStrategyEnv_PPO`

## ğŸ¯ éªŒè¯ç»“æœè¯„ä¼°

### å›æµ‹æ€§èƒ½æŒ‡æ ‡
åœ¨æœ€æ–°æ•°æ®ä¸Šçš„éªŒè¯ç»“æœæ˜¾ç¤ºç­–ç•¥å…·å¤‡è‰¯å¥½çš„å®æˆ˜èƒ½åŠ›ï¼š
![](../plot/4_1_3.png)
![](../plot/4_1_4.png)
**æ”¶ç›Šè¡¨ç°ï¼š**
- **æœ€ç»ˆæ”¶ç›Š**ï¼š1,295.00ï¼ˆåˆå§‹èµ„é‡‘1,000,000ï¼‰
- **æ”¶ç›Šç‡**ï¼š0.13%
- **å¤æ™®æ¯”ç‡**ï¼š0.1807ï¼ˆæ­£æ•°è¡¨æ˜é£é™©è°ƒæ•´åæ”¶ç›Šè‰¯å¥½ï¼‰

**é£é™©æ§åˆ¶ï¼š**
- **æœ€å¤§å›æ’¤**ï¼š0.0579%ï¼ˆæä½ï¼Œé£é™©æ§åˆ¶ä¼˜ç§€ï¼‰
- **é£é™©æ”¶ç›Šæ¯”**ï¼š0.0114
- **ç›ˆåˆ©å› å­**ï¼š1.0489ï¼ˆå¤§äº1è¡¨æ˜ç›ˆåˆ©èƒ½åŠ›å¼ºï¼‰

**äº¤æ˜“ç»Ÿè®¡ï¼š**
- **èƒœç‡**ï¼š51.57%ï¼ˆç•¥é«˜äºéšæœºæ°´å¹³ï¼‰
- **äº¤æ˜“æ¬¡æ•°**ï¼š3,938æ¬¡ï¼ˆ2,031èƒœï¼Œ1,907è´Ÿï¼‰
- **å¹³å‡ç›ˆåˆ©**ï¼š13.68 vs **å¹³å‡äºæŸ**ï¼š13.89
- **ç›ˆäºæ¯”æ¥è¿‘1:1**ï¼Œä½†èƒœç‡ä¼˜åŠ¿å¸¦æ¥æ•´ä½“ç›ˆåˆ©

## ğŸš€ æ•´ä½“è¯„ä»·

### ä¼˜åŠ¿äº®ç‚¹
1. **è®­ç»ƒæ•ˆæœæ˜¾è‘—** - å›æŠ¥æŒ‡æ ‡æŒç»­æ”¹å–„ï¼Œå­¦ä¹ æ›²çº¿è‰¯å¥½
2. **é£é™©æ§åˆ¶å‡ºè‰²** - æä½çš„æœ€å¤§å›æ’¤æ˜¾ç¤ºä¼˜ç§€çš„é£é™©ç®¡ç†
3. **ç¨³å®šæ€§å¼º** - è®­ç»ƒè¿‡ç¨‹å¹³ç¨³ï¼Œæ— å‰§çƒˆæ³¢åŠ¨
4. **å®æˆ˜æœ‰æ•ˆ** - éªŒè¯é›†ä¸Šè·å¾—ç¨³å®šæ­£æ”¶ç›Š

### æ”¹è¿›ç©ºé—´
1. **æ”¶ç›Šç‡æœ‰å¾…æå‡** - è™½ç„¶ç¨³å®šä½†ç»å¯¹æ”¶ç›Šç›¸å¯¹ä¿å®ˆ
2. **èƒœç‡æå‡ç©ºé—´** - 51.57%çš„èƒœç‡ä»æœ‰ä¼˜åŒ–ä½™åœ°
3. **è®­ç»ƒæ•ˆç‡** - å¯è¿›ä¸€æ­¥ä¼˜åŒ–è¶…å‚æ•°æå‡æ”¶æ•›é€Ÿåº¦

## ğŸ’¡ å»ºè®®ä¸å±•æœ›

**çŸ­æœŸä¼˜åŒ–ï¼š**
- è°ƒæ•´å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œé¼“åŠ±æ›´ç§¯æçš„äº¤æ˜“è¡Œä¸º
- ä¼˜åŒ–åŠ¨ä½œç©ºé—´è®¾è®¡ï¼Œæé«˜å†³ç­–ç²¾åº¦
- å¢åŠ ç‰¹å¾å·¥ç¨‹å¤æ‚åº¦ï¼ŒæŒ–æ˜æ›´å¤šæœ‰æ•ˆä¿¡å·

**é•¿æœŸå‘å±•ï¼š**
- å¼•å…¥å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾
- ç»“åˆå¸‚åœºçŠ¶æ€è¯†åˆ«æ¨¡å—
- å¼€å‘åŠ¨æ€ä»“ä½ç®¡ç†æœºåˆ¶

è¯¥å¼ºåŒ–å­¦ä¹ ç­–ç•¥å±•ç°äº†è‰¯å¥½çš„åŸºç¡€æ€§èƒ½ï¼Œä¸ºåç»­æ›´å¤æ‚çš„äº¤æ˜“ç­–ç•¥å¼€å‘å¥ å®šäº†åšå®åŸºç¡€ã€‚ğŸ¯



## 7. æœ€ä½³å®è·µ

### 7.1 ç‰¹å¾å·¥ç¨‹å»ºè®®

1. **ç‰¹å¾é€‰æ‹©**ï¼š
   - ä½¿ç”¨ç›¸å…³æ€§åˆ†æé€‰æ‹©ä¸ç›®æ ‡ç›¸å…³çš„ç‰¹å¾
   - é¿å…ä½¿ç”¨æœªæ¥æ•°æ®
   - è€ƒè™‘å¸‚åœº regime çš„å˜åŒ–

2. **å½’ä¸€åŒ–å¤„ç†**ï¼š
   - è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–å‚æ•°
   - è€ƒè™‘ä½¿ç”¨æ»šåŠ¨çª—å£å½’ä¸€åŒ–é¿å…æœªæ¥ä¿¡æ¯æ³„éœ²

3. **æ•°æ®å¢å¼º**ï¼š
   - é€‚å½“çš„æ•°æ®å¢å¼ºå¯ä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
   - ä½†é¿å…è¿‡åº¦å¢å¼ºå¯¼è‡´ä¿¡å·å¤±çœŸ

### 7.2 è®­ç»ƒæŠ€å·§

1. **å¥–åŠ±è®¾è®¡**ï¼š
   - å¥–åŠ±å‡½æ•°åº”è¯¥ä¸æœ€ç»ˆç›®æ ‡ä¸€è‡´
   - è€ƒè™‘é£é™©è°ƒæ•´åçš„æ”¶ç›Š
   - é¿å…ç¨€ç–å¥–åŠ±é—®é¢˜

2. **è¶…å‚æ•°è°ƒä¼˜**ï¼š
   - ä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–
   - é‡ç‚¹å…³æ³¨å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€æ‰¹æ¬¡å¤§å°

3. **æ¨¡å‹éªŒè¯**ï¼š
   - ä½¿ç”¨æ ·æœ¬å¤–æ•°æ®éªŒè¯
   - è€ƒè™‘å¤šä¸ªå¸‚åœºç¯å¢ƒçš„æµ‹è¯•
   - ç›‘æ§è¿‡æ‹Ÿåˆç°è±¡

### 7.3 é£é™©ç®¡ç†

1. **ä»“ä½ç®¡ç†**ï¼š
   - åœ¨åŠ¨ä½œè®¾è®¡ä¸­è€ƒè™‘ä»“ä½æ§åˆ¶
   - é¿å…è¿‡åº¦æ æ†

2. **æ­¢æŸæœºåˆ¶**ï¼š
   - åœ¨ç¯å¢ƒä¸­å®ç°æ­¢æŸé€»è¾‘
   - è€ƒè™‘æ³¢åŠ¨ç‡è°ƒæ•´çš„æ­¢æŸ

é€šè¿‡ä»¥ä¸ŠæŒ‡å—ï¼Œæ‚¨å¯ä»¥å……åˆ†åˆ©ç”¨ minibt æ¡†æ¶çš„å¼ºåŒ–å­¦ä¹ åŠŸèƒ½ï¼Œæ„å»ºé«˜æ•ˆçš„é‡åŒ–äº¤æ˜“ç­–ç•¥ã€‚è®°å¾—åœ¨å®é™…ä½¿ç”¨ä¸­æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´å‚æ•°å’Œé€»è¾‘ã€‚

> é£é™©æç¤ºï¼šæœ¬æ–‡æ¶‰åŠçš„äº¤æ˜“ç­–ç•¥ã€ä»£ç ç¤ºä¾‹å‡ä¸ºæŠ€æœ¯æ¼”ç¤ºã€æ•™å­¦æ¢è®¨ï¼Œä»…ç”¨äºå±•ç¤ºé€»è¾‘æ€è·¯ï¼Œç»ä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€æ“ä½œæŒ‡å¼•æˆ–å†³ç­–ä¾æ® ã€‚é‡‘èå¸‚åœºå¤æ‚å¤šå˜ï¼Œå­˜åœ¨ä»·æ ¼æ³¢åŠ¨ã€æ”¿ç­–è°ƒæ•´ã€æµåŠ¨æ€§ç­‰å¤šé‡é£é™©ï¼Œå†å²è¡¨ç°ä¸é¢„ç¤ºæœªæ¥ç»“æœã€‚ä»»ä½•äº¤æ˜“å†³ç­–å‡éœ€æ‚¨è‡ªä¸»åˆ¤æ–­ã€ç‹¬ç«‹æ‰¿æ‹…è´£ä»» â€”â€” è‹¥ä¾æ®æœ¬æ–‡å†…å®¹æ“ä½œï¼Œç›ˆäºåæœæ¦‚ç”±è‡ªèº«æ‰¿æ‹…ã€‚è¯·åŠ¡å¿…å……åˆ†è¯„ä¼°é£é™©æ‰¿å—èƒ½åŠ›ï¼Œç†æ€§å¯¹å¾…å¸‚åœºï¼Œè°¨æ…åšå‡ºæŠ•èµ„é€‰æ‹©ã€‚